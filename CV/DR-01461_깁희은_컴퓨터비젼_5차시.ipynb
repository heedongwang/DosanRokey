{"cells":[{"cell_type":"markdown","metadata":{"id":"nmq_ohyPB_im"},"source":["# 답안 작성 방법\n","\n","아래 이미지에서 \"더블클릭 또는 Enter키를 눌러 수정\"을 누르신후 해당 창에 답을 적으시면 됩니다.\n","\n","![image](https://github.com/user-attachments/assets/2aa2ff05-fb0e-4f00-a121-78afeaad4f09)\n","\n","<br>\n","<br>\n","<br>\n","<br>\n","<br>\n","<br>\n","<br>\n","<br>\n","<br>"]},{"cell_type":"markdown","metadata":{"id":"JiAFUXF2DadY"},"source":["# 05차시 과제"]},{"cell_type":"markdown","metadata":{"id":"gL_mLiXouq9t"},"source":["### 1. Logistic Regression이 무엇인지 서술하시오."]},{"cell_type":"markdown","metadata":{"id":"Xz7UXDATuq9u"},"source":["이진 분류를 해결하기위해 사용됩니다. 0,1만 출력되게 합니다"]},{"cell_type":"markdown","metadata":{"id":"ZcoGzAp6uq9u"},"source":["### 2. Logistic Regression을 사용할 수 있는 사례를 5가지만 서술하시오."]},{"cell_type":"markdown","metadata":{"id":"zSX5CvLUuq9v"},"source":["1. 금융(주가) 등 예측\n","2. 날씨 분석 예측\n","3. 신용카드 거래 예측\n","4. 광고 클릭 예측\n","5. 의료 진단"]},{"cell_type":"markdown","metadata":{"id":"vTmT0IFuuq9v"},"source":["### 3. 제시된 코드에 주석을 작성하고 조건에 맞추어 빈칸(***)를 채워라.\n","\n","**조건**\n","\n","1. Optimizer는 Adam을 사용하라\n","2. learning rate는 0.001을 사용하라\n","3. loss function은 MSE를 사용하라.\n","4. 에포크는 100으로 설정하라."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"id":"do8WOpPfuq9w","executionInfo":{"status":"ok","timestamp":1724146122957,"user_tz":-540,"elapsed":299,"user":{"displayName":"wow wow","userId":"00794009737200249602"}},"outputId":"f4972325-d3e2-4cfe-dd57-2a7cc2db9ffb"},"outputs":[{"output_type":"stream","name":"stdout","text":["epoch: 0, loss: 24.586572647094727\n","epoch: 1, loss: 24.530607223510742\n","epoch: 2, loss: 24.474706649780273\n","epoch: 3, loss: 24.418874740600586\n","epoch: 4, loss: 24.36311149597168\n","epoch: 5, loss: 24.30741310119629\n","epoch: 6, loss: 24.25179100036621\n","epoch: 7, loss: 24.19624137878418\n","epoch: 8, loss: 24.140762329101562\n","epoch: 9, loss: 24.085357666015625\n","epoch: 10, loss: 24.030029296875\n","epoch: 11, loss: 23.974777221679688\n","epoch: 12, loss: 23.919601440429688\n","epoch: 13, loss: 23.864501953125\n","epoch: 14, loss: 23.809484481811523\n","epoch: 15, loss: 23.754545211791992\n","epoch: 16, loss: 23.69968605041504\n","epoch: 17, loss: 23.644906997680664\n","epoch: 18, loss: 23.590211868286133\n","epoch: 19, loss: 23.535600662231445\n","epoch: 20, loss: 23.48107147216797\n","epoch: 21, loss: 23.42662811279297\n","epoch: 22, loss: 23.372268676757812\n","epoch: 23, loss: 23.3179931640625\n","epoch: 24, loss: 23.263809204101562\n","epoch: 25, loss: 23.209707260131836\n","epoch: 26, loss: 23.155691146850586\n","epoch: 27, loss: 23.10176658630371\n","epoch: 28, loss: 23.047927856445312\n","epoch: 29, loss: 22.994176864624023\n","epoch: 30, loss: 22.940515518188477\n","epoch: 31, loss: 22.886947631835938\n","epoch: 32, loss: 22.833463668823242\n","epoch: 33, loss: 22.780073165893555\n","epoch: 34, loss: 22.726770401000977\n","epoch: 35, loss: 22.673559188842773\n","epoch: 36, loss: 22.620439529418945\n","epoch: 37, loss: 22.567413330078125\n","epoch: 38, loss: 22.51447105407715\n","epoch: 39, loss: 22.461624145507812\n","epoch: 40, loss: 22.40886878967285\n","epoch: 41, loss: 22.3562068939209\n","epoch: 42, loss: 22.303634643554688\n","epoch: 43, loss: 22.25115394592285\n","epoch: 44, loss: 22.198766708374023\n","epoch: 45, loss: 22.146469116210938\n","epoch: 46, loss: 22.094268798828125\n","epoch: 47, loss: 22.042158126831055\n","epoch: 48, loss: 21.990137100219727\n","epoch: 49, loss: 21.93821144104004\n","epoch: 50, loss: 21.886377334594727\n","epoch: 51, loss: 21.83463478088379\n","epoch: 52, loss: 21.782983779907227\n","epoch: 53, loss: 21.731428146362305\n","epoch: 54, loss: 21.679962158203125\n","epoch: 55, loss: 21.628591537475586\n","epoch: 56, loss: 21.57731056213379\n","epoch: 57, loss: 21.526123046875\n","epoch: 58, loss: 21.475027084350586\n","epoch: 59, loss: 21.424026489257812\n","epoch: 60, loss: 21.37311363220215\n","epoch: 61, loss: 21.322294235229492\n","epoch: 62, loss: 21.27156639099121\n","epoch: 63, loss: 21.220932006835938\n","epoch: 64, loss: 21.170387268066406\n","epoch: 65, loss: 21.119937896728516\n","epoch: 66, loss: 21.069578170776367\n","epoch: 67, loss: 21.019311904907227\n","epoch: 68, loss: 20.969133377075195\n","epoch: 69, loss: 20.919050216674805\n","epoch: 70, loss: 20.869056701660156\n","epoch: 71, loss: 20.819154739379883\n","epoch: 72, loss: 20.769346237182617\n","epoch: 73, loss: 20.719629287719727\n","epoch: 74, loss: 20.670000076293945\n","epoch: 75, loss: 20.62046241760254\n","epoch: 76, loss: 20.571016311645508\n","epoch: 77, loss: 20.52166175842285\n","epoch: 78, loss: 20.47239875793457\n","epoch: 79, loss: 20.4232234954834\n","epoch: 80, loss: 20.374143600463867\n","epoch: 81, loss: 20.325149536132812\n","epoch: 82, loss: 20.276247024536133\n","epoch: 83, loss: 20.22743797302246\n","epoch: 84, loss: 20.1787166595459\n","epoch: 85, loss: 20.130084991455078\n","epoch: 86, loss: 20.08154296875\n","epoch: 87, loss: 20.03309440612793\n","epoch: 88, loss: 19.984731674194336\n","epoch: 89, loss: 19.936460494995117\n","epoch: 90, loss: 19.888280868530273\n","epoch: 91, loss: 19.840187072753906\n","epoch: 92, loss: 19.79218292236328\n","epoch: 93, loss: 19.74427032470703\n","epoch: 94, loss: 19.69644546508789\n","epoch: 95, loss: 19.648710250854492\n","epoch: 96, loss: 19.601064682006836\n","epoch: 97, loss: 19.553508758544922\n","epoch: 98, loss: 19.506040573120117\n","epoch: 99, loss: 19.45865821838379\n"]}],"source":["import torch\n","\n","model = torch.nn.Linear(2, 1)\n","\n","optim = torch.optim.Adam(model.parameters(), lr=0.001)\n","criterion = torch.nn.MSELoss()\n","\n","x_data = torch.Tensor([[1.0, 1.5], [2.0, 3.0], [2.0, 4.0]])\n","y_data = torch.Tensor([[2.0], [4.0], [6.0]])\n","\n","for epoch in range(100):\n","    y_pred = model(x_data)\n","\n","    loss = criterion(y_pred, y_data)\n","\n","    optim.zero_grad()\n","    loss.backward()\n","    optim.step()\n","    print(f\"epoch: {epoch}, loss: {loss.item()}\")"]},{"cell_type":"markdown","metadata":{"id":"SV5ZCwphuq9x"},"source":["### 4. 역전파(Back-propagation)이 무엇인지 서술하시오."]},{"cell_type":"markdown","metadata":{"id":"bRcBcE5Huq9y"},"source":["오차를 역방향으로 전달하면서 가중치를 업데이트 합니다.이 과정을 통해 최적화됩니다."]},{"cell_type":"markdown","metadata":{"id":"9Y7nTm3iuq9y"},"source":["### 5. 아래 제공된 수식은 sigmoid 함수이다. z값이 양의 무한대로 발산할때와 음의 무한대로 발산할 때 각각 sigmoid 값이 어떻게 변하는지 서술하라.\n","\n","\n","$$\n","\n","\\sigma(z) = {1 \\over 1+e^{-z}}\n","\n","$$\n","\n"]},{"cell_type":"markdown","metadata":{"id":"WVfbCmyxuq9y"},"source":["+무한대: 1에 근접합니다 /\n","-무한대: 0에 근접합니다."]},{"cell_type":"markdown","metadata":{"id":"YyB_nhMIuq9y"},"source":["### 6. Sigmoid 함수의 입력값(z)이 양의 무한대로 발산하거나 음의 무한대로 발산하는 경우 인공지능 모델의 학습에 어떠한 영향을 미치는지 서술하라.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"D1if6VCRuq9z"},"source":["양의 무한대: 1에 가까워지며, 과적합이 발생할 수 있습니다. 특정 클래스에만 치우칩니다.\n","\n","음의 무한대 : 0에 가까워 지며,과소적합이 발생 할 수 있습니다. 제대로 분류하지 못합니다."]},{"cell_type":"markdown","metadata":{"id":"w6VuTTNHuq9z"},"source":["### 7. 아래 제공된 수식은 cross entropy loss에 관한 수식이다. y = [1, 1, 0, 0]이고  $\\hat{y}$은 [1.0, 0.25, 0.875, 0.0]일 때의 loss 값을 구하라.\n","\n","* 아래 수식에서 log의 밑은 2라고 가정한다.\n","$$\n","    loss = - {1 \\over N} log \\hat{y} \\sum_{n=1}^{N}y_n log\\hat{y}_n + (1- y_n)log (1-\\hat{y}_n)\n","$$"]},{"cell_type":"code","source":["import math\n","y = [1, 1, 0, 0]\n","yh=[1.0,0.25,0.875,0.0]\n","# loss1=1*math.log2(1.0)+(1-1)*math.log2(1-1.0)  =0\n","loss2=(y[1]*math.log2(yh[1])+(1-y[1])*math.log2(1-yh[1]))\n","loss3=(y[2]*math.log2(yh[2])+(1-y[2])*math.log2(1-yh[2]))\n","# loss4=(y[3]*math.log2(yh[3])+(1-y[3])*math.log2(1-yh[3])) =0\n","loss=loss2+loss3\n","print((-1/4)*loss)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cfE9NHz6H5tq","executionInfo":{"status":"ok","timestamp":1724147155781,"user_tz":-540,"elapsed":331,"user":{"displayName":"wow wow","userId":"00794009737200249602"}},"outputId":"09bb3b0f-8f02-43cb-e895-f2075ce3a15b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["1.25\n"]}]},{"cell_type":"markdown","metadata":{"id":"JaNkxx5Nuq9z"},"source":["### 8. 아래 제공된 두 행렬식의 곱셈 결과를 구하라.\n","\n","$$\n","\n","A = \\begin{bmatrix} 1 & 2 \\\\ 3 & 4 \\end{bmatrix}\n","\\qquad\n","B = \\begin{bmatrix} 4 & 3 \\\\ 2 & 1 \\end{bmatrix}\n","$$\n","\n","$$\n","A \\times B = ?\n","$$"]},{"cell_type":"markdown","source":["A×B= \\begin{bmatrix} 13 & 5 \\\\ 20 & 8 \\end{bmatrix}"],"metadata":{"id":"Oz5vfZwqzd_i"}},{"cell_type":"markdown","metadata":{"id":"lrAQzceZuq90"},"source":["### 9. 아래 코드의 output을 확인한 후 행렬곱과 완전 연결 신경망(fully-connected layer)의 관계에 대해 서술하라."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"foqu8M4cuq90","executionInfo":{"status":"ok","timestamp":1724145557045,"user_tz":-540,"elapsed":319,"user":{"displayName":"wow wow","userId":"00794009737200249602"}},"outputId":"b0ff4900-9773-4168-9a17-1ac286bb4564"},"outputs":[{"output_type":"stream","name":"stdout","text":["A와 B의 행렬 곱 :  tensor([[ 9.,  5.],\n","        [25., 14.]])\n","tensor([[5., 2.],\n","        [3., 1.]])\n","A를 linear layer W에 통과시킨 output 값 tensor([[ 9.,  5.],\n","        [25., 14.]], grad_fn=<MmBackward0>)\n"]}],"source":["import torch\n","\n","A = torch.Tensor([[1, 2], [3, 5]])\n","B = torch.Tensor([[5, 3], [2, 1]])\n","\n","print(\"A와 B의 행렬 곱 : \", torch.matmul(A, B))\n","\n","W = linear = torch.nn.Linear(2, 2, bias=False)\n","W.weight = torch.nn.Parameter(B.T)\n","print(\"A를 linear layer W에 통과시킨 output 값\", W(A))"]},{"cell_type":"markdown","source":["B를 전치하면 B.T=\\begin{bmatrix} 5 & 2 \\\\ 3 & 1 \\end{bmatrix}\n","\n","레이어를 통과하는 것은 AXB.T와 같습니다.\n","따라서 완전 연결 신경망의 주요 연산은 입력 벡터와 가중치 행렬의 행렬곱입니다\n","\n"],"metadata":{"id":"JLdEfGHB8kNa"}},{"cell_type":"markdown","metadata":{"id":"APMH8tcWuq90"},"source":["### 10. BatchSize와 Epoch이 무엇인지 각각 서술하시오."]},{"cell_type":"markdown","metadata":{"id":"OOjAWPP3uq90"},"source":["BatchSize: 한 번 학습에서 신경망이 처리하는 데이터의 개수입니다. 학습속도, 선능에 영향을 미칩니다.\n","Epoch: 전체 데이터셋을 한 번 완전히 학습하는 과정의 개수입니다.\n","너무 크거나 작은 에폭을 가지게 되면 과적합, 과소적합이 발생할 수 있습니다."]},{"cell_type":"markdown","metadata":{"id":"RhWVjxc4uq91"},"source":["### 11. 제공된 조건에 맞추어 아래 코드의 빈칸(***)을 채우고 코드가 오류없이 돌아가는지 확인하시오.\n","\n","**조건**\n","* 모델은 4개의 linear layer로 구성되어 있다.\n","* 각 linear layer의 input과 output은 다음과 같다.\n","    * linear1\n","        * input: 1\n","        * output: 3\n","    * linear2\n","        * input: 3\n","        * output: 5\n","    * linear3\n","        * input: 5\n","        * output: 5\n","    * linear4\n","        * input: 5\n","        * output: 1\n","\n","* model의 layer 구성은 다음과 같다.\n","    * linear1 --> relu --> linear2 --> relu --> linear3 --> relu --> linear4"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ghXtS1vYuq91","executionInfo":{"status":"ok","timestamp":1724143222476,"user_tz":-540,"elapsed":572,"user":{"displayName":"wow wow","userId":"00794009737200249602"}},"outputId":"1b7eb315-4fce-4333-dade-d56481d9e781"},"outputs":[{"output_type":"stream","name":"stdout","text":["Model(\n","  (linear1): Linear(in_features=1, out_features=3, bias=True)\n","  (linear2): Linear(in_features=3, out_features=5, bias=True)\n","  (linear3): Linear(in_features=5, out_features=5, bias=True)\n","  (linear4): Linear(in_features=5, out_features=1, bias=True)\n","  (relu): ReLU()\n",")\n","tensor([0.1736], grad_fn=<ViewBackward0>)\n"]}],"source":["import torch\n","\n","class Model(torch.nn.Module):\n","    def __init__(self):\n","        super(Model, self).__init__()\n","        self.linear1 = torch.nn.Linear(1, 3)\n","        self.linear2 = torch.nn.Linear(3, 5)\n","        self.linear3 = torch.nn.Linear(5, 5)\n","        self.linear4 = torch.nn.Linear(5, 1)\n","\n","        self.relu = torch.nn.ReLU()\n","    def forward(self, x):\n","        x = self.relu(self.linear1(x))\n","        x = self.relu(self.linear2(x))\n","        x = self.relu(self.linear3(x))\n","        x = self.linear4(x)\n","        return x\n","\n","model = Model()\n","print(model)\n","\n","x = torch.Tensor([1.0])\n","print(model(x))\n","\n"]},{"cell_type":"markdown","metadata":{"id":"n8Ueo2Q2uq91"},"source":["### 12.  DataLoader가 무엇이고 왜 필요한지 서술하시오"]},{"cell_type":"markdown","metadata":{"id":"iYUq5bKguq91"},"source":["데이터를 나누어 모델에 효율적으로 공급합니다, 관리를 쉽게하고 모델 학습에 적합한 형식으로 제공하기 위해 사용합니다.\n","\n","필요한 이유:\n","1. 효율적인 데이터 처리\n","2. 다양한 데이터 접근\n","3. 메모리관리\n","4. 편리한 데이터 전처리"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.4"}},"nbformat":4,"nbformat_minor":0}